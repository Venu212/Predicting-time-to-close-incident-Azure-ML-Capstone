{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## Azure-related\n",
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create compute cluster\n",
    "        # Get WS details and Subscription details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T05:24:06.340805Z",
     "start_time": "2021-01-03T05:24:06.312783Z"
    },
    "gather": {
     "logged": 1609464140003
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azureml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-5756c8386e46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#ws = Workspace.get(name=\"udacity-project\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'azureml'"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Experiment, Run\n",
    "#ws = Workspace.get(name=\"udacity-project\")\n",
    "ws = Workspace.from_config() \n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a name for experiment\n",
    "experiment_name = 'incidents-exp'\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "run = exp.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specify compute targets\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that the cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',\n",
    "                                                           idle_seconds_before_scaledown=2400,\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "* The incident management log data is retrieved from UCI Machine learnig data set. [Incident Data)[https://archive.ics.uci.edu/ml/datasets/Incident+management+process+enriched+event+log]. The data is sourced from a Servicenow platform for IT service management.  \n",
    "  \n",
    "* This provides details of various incidents recorded over period of time . I have taken up this project to predict ETA(expected time of accomplishment) to understand time to resolve each incident. This helps IT department to provide ETA for customers based on time taken to resolve similar issues historically. This will also help IT department to understand if any instance will go beyond expected SLA.\n",
    "\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "from azureml.core import Dataset\n",
    "\n",
    "datasource = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00498/incident_event_log.zip'\n",
    "dataset = Dataset.Tabular.from_delimited_files(path=datasource, header=True).to_pandas_dataframe()\n",
    "#dataset.columns =['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'iris_class'] \n",
    "\n",
    "print(dataset.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import source.loadData as src\n",
    "import source.loadData as src\n",
    "import pandas as pd\n",
    "\n",
    "datasource = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00498/incident_event_log.zip\"\n",
    "df = src.loadData(datasource)\n",
    "#df.head(2)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T04:23:15.449667Z",
     "start_time": "2021-01-03T04:23:15.407666Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>incident_state</th>\n",
       "      <th>active</th>\n",
       "      <th>reassignment_count</th>\n",
       "      <th>reopen_count</th>\n",
       "      <th>sys_mod_count</th>\n",
       "      <th>made_sla</th>\n",
       "      <th>caller_id</th>\n",
       "      <th>opened_by</th>\n",
       "      <th>opened_at</th>\n",
       "      <th>...</th>\n",
       "      <th>problem_id</th>\n",
       "      <th>rfc</th>\n",
       "      <th>vendor</th>\n",
       "      <th>caused_by</th>\n",
       "      <th>closed_code</th>\n",
       "      <th>resolved_by</th>\n",
       "      <th>resolved_at</th>\n",
       "      <th>closed_at</th>\n",
       "      <th>closedat</th>\n",
       "      <th>days_to_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INC0000045</td>\n",
       "      <td>New</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Caller 2403</td>\n",
       "      <td>Opened by  8</td>\n",
       "      <td>2016-02-29 01:16:00</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>code 5</td>\n",
       "      <td>Resolved by 149</td>\n",
       "      <td>29/2/2016 11:29</td>\n",
       "      <td>2016-05-03 12:00:00</td>\n",
       "      <td>2016-05-03 12:00:00</td>\n",
       "      <td>64 days 10:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INC0000045</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>Caller 2403</td>\n",
       "      <td>Opened by  8</td>\n",
       "      <td>2016-02-29 01:16:00</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>code 5</td>\n",
       "      <td>Resolved by 149</td>\n",
       "      <td>29/2/2016 11:29</td>\n",
       "      <td>2016-05-03 12:00:00</td>\n",
       "      <td>2016-05-03 12:00:00</td>\n",
       "      <td>64 days 10:44:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       number incident_state  active  reassignment_count  reopen_count  \\\n",
       "0  INC0000045            New    True                   0             0   \n",
       "1  INC0000045       Resolved    True                   0             0   \n",
       "\n",
       "   sys_mod_count  made_sla    caller_id     opened_by           opened_at  \\\n",
       "0              0      True  Caller 2403  Opened by  8 2016-02-29 01:16:00   \n",
       "1              2      True  Caller 2403  Opened by  8 2016-02-29 01:16:00   \n",
       "\n",
       "   ... problem_id rfc vendor caused_by closed_code      resolved_by  \\\n",
       "0  ...          ?   ?      ?         ?      code 5  Resolved by 149   \n",
       "1  ...          ?   ?      ?         ?      code 5  Resolved by 149   \n",
       "\n",
       "       resolved_at           closed_at            closedat    days_to_close  \n",
       "0  29/2/2016 11:29 2016-05-03 12:00:00 2016-05-03 12:00:00 64 days 10:44:00  \n",
       "1  29/2/2016 11:29 2016-05-03 12:00:00 2016-05-03 12:00:00 64 days 10:44:00  \n",
       "\n",
       "[2 rows x 38 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T02:27:10.408461Z",
     "start_time": "2021-01-03T02:27:10.380458Z"
    },
    "gather": {
     "logged": 1609570602785
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import dateutil.parser\n",
    "df['closed_at'] = pd.to_datetime(df['closed_at'])\n",
    "df['opened_at'] = pd.to_datetime(df['opened_at'])\n",
    "\n",
    "df['days_to_close'] = (df['closed_at'] - df['opened_at'])\n",
    "df['days_to_close'].dt.components.days\n",
    "#df['days_to_close'] = df['days_to_close'].date()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T05:22:43.437242Z",
     "start_time": "2021-01-03T05:22:43.433244Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y = src.select_features_target(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T00:17:56.970733Z",
     "start_time": "2021-01-03T00:17:56.955723Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = src.train_test_data(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1609565042086
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#df['opened_at'] = pd.to_datetime(df['opened_at'])\n",
    "#df['sys_created_at'] = pd.to_datetime(df['sys_created_at'])\n",
    "#df['sys_updated_at'] = pd.to_datetime(df['sys_updated_at'])\n",
    "#df['resolved_at'] = pd.to_datetime(df['resolved_at'])\n",
    "#df['opened_at'] = pd.to_datetime(df['opened_at'])\n",
    "#df['closed_at'] = pd.to_datetime(df['closed_at'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1609563179036
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "df.astype({\n",
    "'opened_at':'datetime64',\n",
    "'sys_created_at':'datetime64',\n",
    "'sys_updated_at':'datetime64',\n",
    "'sys_updated_at':'datetime64',\n",
    "'resolved_at':'datetime64',\n",
    "'closed_at':'datatime64'}).dtypes\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "TODO: Explain why you chose the automl settings and cofiguration you used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598429217746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "# TODO: Put your automl settings here\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\"=60,\n",
    "    \"max_concurrent_iterations\":5,\n",
    "    \"primary_metric\"='r2_score',\n",
    "    \"n_cross_validations\"=5,\n",
    "    \n",
    "    \"iteration_timeout_minutes\" : 10,\n",
    "    \"iterations\" : 10,\n",
    "    \"primary_metric\" : 'AUC_weighted',\n",
    "    \"verbosity\" : logging.INFO,\n",
    "    \"preprocess\": True\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Put your automl config here\n",
    "automl_config = AutoMLConfig(task='regression',\n",
    "                             allowed_models=['KNN'],\n",
    "                             label_column_name=label,\n",
    "                             training_data=train_data,\n",
    "                            )\n",
    "'''\n",
    " automated_ml_config = AutoMLConfig(task = 'classification',\n",
    "                                 debug_log = 'automated_ml_errors.log',\n",
    "                                 compute_target=compute_target,\n",
    "                                 path = project_folder,\n",
    "                                 data_script= project_folder + \"/get_data.py\",\n",
    "                                 model_explainability=True,\n",
    "                                 **automl_settings)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Submit your experiment\n",
    "run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# submit an experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "automl_run = exp.submit(automl_config, show_output=True)\n",
    "RunDetails(automl_run).show()\n",
    "automl_run.wait_for_completion(show_output=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERDRIVE\n",
    "\n",
    "1.2  Hyperdrive Configuration¶\n",
    "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.parameter_expressions import uniform, choice\n",
    "import os\n",
    "\n",
    "# Specify parameter sampler\n",
    "ps = RandomParameterSampling(\n",
    "   parameter_space={\n",
    "       \"--C\": uniform(0.1, 0.9),\n",
    "       \"--max_iter\": choice(10, 50, 100)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Specify a Policy\n",
    "policy = BanditPolicy(\n",
    "    evaluation_interval=3,\n",
    "    slack_factor=0.1\n",
    ")\n",
    "\n",
    "# Create a SKLearn estimator for use with train.py\n",
    "est = SKLearn(\n",
    "    source_directory=\"./\",\n",
    "    entry_script=\"train.py\",\n",
    "    compute_target=aml_compute,\n",
    ")\n",
    "\n",
    "# Create a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.\n",
    "hyperdrive_config = HyperDriveConfig(\n",
    "    primary_metric_name=\"Accuracy\",\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "    max_total_runs=48,\n",
    "    max_concurrent_runs=8,\n",
    "    hyperparameter_sampling=ps,\n",
    "    policy=policy,\n",
    "    estimator=est\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Hyperdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(hyperdrive_run).show()\n",
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "print(best_run)\n",
    "print(hyperdrive_run.get_metrics())\n",
    "\n",
    "\n",
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(hyperdrive_run.get_status() == \"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_automl_run, best_model = automl_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get your evaluation metrics as a dictionary, use the following commamd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_metrics = run.get_metrics()\n",
    "run_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_manual_accuracy = float(max(run_metrics['Accuracy'])) * 100 \n",
    "print(\"The best accuracy we acheived with Scikit Learn:  %.2f%%\" % best_manual_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import get_run\n",
    "run_cpu_id = 'AutoML_2e02b696-bf8a-47f4-b035-64347532b27e_9'\n",
    "# run_cpu_id = 'AutoML_fd9055d1-1f4b-4484-ae70-d773ca82bd67' #get from portal\n",
    "best_run = get_run(experiment, run_cpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name = \"best_run_automl.pkl\", model_path = './outputs/')\n",
    "\n",
    "\n",
    "# az ml model register -n sklearn_mnist  --asset-path outputs/sklearn_mnist_model.pkl  --experiment-name myexperiment --run-id myrunid --tag area=mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model._final_estimator)\n",
    "print(best_automl_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#az ml model deploy --ic inferenceconfig.json --dc deploymentconfig.json\n",
    "\n",
    "\n",
    "from azureml.core import Model\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "\n",
    "model = best_run.register_model(model_name='hyperdrive_model', model_path='./outputs/model.pkl',\n",
    "                       model_framework=Model.Framework.SCIKITLEARN,\n",
    "                       model_framework_version='0.19.1')\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=2,\n",
    "                                                enable_app_insights=True, auth_enabled=True)  \n",
    "service_name = 'my-sklearn-service'\n",
    "service = Model.deploy(ws, service_name, [model], deployment_config=aci_config)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "\n",
    "print(\"You can perform HTTP POST on URL {} to trigger this pipeline\".format(rest_endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# specify the param when running the pipeline\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=aad_token, \n",
    "                         json={\"ExperimentName\": \"MyRestPipeline\",\n",
    "                               \"RunSource\": \"SDK\",\n",
    "                               \"DataSetDefinitionValueAssignments\": {\"file_ds_param\": {\"SavedDataSetReference\": {\"Id\": iris_file_ds.id}},\n",
    "                                                                     \"tabular_ds_param\": {\"SavedDataSetReference\": {\"Id\": iris_tabular_ds.id}}}\n",
    "                              }\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception('Received bad response from the endpoint: {}\\n'\n",
    "                    'Response Code: {}\\n'\n",
    "                    'Headers: {}\\n'\n",
    "                    'Content: {}'.format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print('Submitted pipeline run: ', run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline_run_via_rest = PipelineRun(ws.experiments[\"MyRestPipeline\"], run_id)\n",
    "RunDetails(published_pipeline_run_via_rest).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline_run_via_rest.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and use ONNX Runtime with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install onnxruntime\t      # CPU build\n",
    "#pip install onnxruntime-gpu   # GPU build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "session = onnxruntime.InferenceSession(\"path to model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_modelmeta()\n",
    "first_input_name = session.get_inputs()[0].name\n",
    "first_output_name = session.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = session.run([\"output1\", \"output2\"], {\n",
    "                      \"input1\": indata1, \"input2\": indata2})\n",
    "results = session.run([], {\"input1\": indata1, \"input2\": indata2})"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
